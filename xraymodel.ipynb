{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ca1a73-0a93-4df0-9e69-6b1efa33b80d",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-10-26T15:35:51.060646Z",
     "iopub.status.busy": "2025-10-26T15:35:51.060426Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ File exists and is accessible!\n",
      "Loaded 7725 rows from CSV\n",
      "training\n",
      "<class '__main__.ConvNet'>\n",
      "True\n",
      "True\n",
      "Epoch  {1}\n"
     ]
    }
   ],
   "source": [
    "# train.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import boto3, io, pandas as pd\n",
    "from PIL import Image\n",
    "from botocore.exceptions import ClientError\n",
    "import time, matplotlib.pyplot as plt, numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "# --- AWS setup ---\n",
    "REGION = \"us-east-1\"\n",
    "BUCKET = \"aws-chest-xray-model-hackathon\"\n",
    "IMAGES_PREFIX = \"archive\"\n",
    "LABELS_PATH = f\"s3://{BUCKET}/archive/effusion.csv\"\n",
    "\n",
    "s3 = boto3.client(\"s3\", region_name=REGION)\n",
    "\n",
    "test_key = \"archive/00001389_004.png\"\n",
    "try:\n",
    "    s3.head_object(Bucket=BUCKET, Key=test_key)\n",
    "    print(\"‚úÖ File exists and is accessible!\")\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Error fetching file:\", e)\n",
    "\n",
    "# --- Load CSV ---\n",
    "labels_df = pd.read_csv(LABELS_PATH)\n",
    "labels_df.rename(columns={\"Image Index\": \"image_name\", \"Label\": \"label\"}, inplace=True)\n",
    "print(f\"Loaded {len(labels_df)} rows from CSV\")\n",
    "\n",
    "\n",
    "# --- Transform ---\n",
    "img_size = 256\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# --- Dataset ---\n",
    "class S3ImageDataset(Dataset):\n",
    "    def __init__(self, df, bucket, prefix, s3_client, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.bucket = bucket\n",
    "        self.prefix = prefix\n",
    "        self.s3 = s3_client\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image_name = str(row[\"image_name\"])\n",
    "        label = torch.tensor(row[\"label\"], dtype=torch.float32)\n",
    "        key = f\"{self.prefix}/{image_name}\"\n",
    "\n",
    "        try:\n",
    "            self.s3.head_object(Bucket=self.bucket, Key=key)\n",
    "        except ClientError:\n",
    "            return None\n",
    "\n",
    "        obj = self.s3.get_object(Bucket=self.bucket, Key=key)\n",
    "        img_bytes = obj[\"Body\"].read()\n",
    "        img = Image.open(io.BytesIO(img_bytes)).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "def collate_skip_missing(batch):\n",
    "    batch = [x for x in batch if x is not None]\n",
    "    if len(batch) == 0:\n",
    "        return None\n",
    "    imgs, labels = zip(*batch)\n",
    "    return torch.stack(imgs, 0), torch.tensor(labels)\n",
    "\n",
    "# --- Split ---\n",
    "train_df, val_df = train_test_split(labels_df, test_size=0.2, stratify=labels_df[\"label\"], random_state=42)\n",
    "\n",
    "trainloader = DataLoader(\n",
    "    S3ImageDataset(train_df, BUCKET, IMAGES_PREFIX, s3, transform),\n",
    "    batch_size=32, shuffle=True, num_workers=2, collate_fn=collate_skip_missing)\n",
    "\n",
    "valloader = DataLoader(\n",
    "    S3ImageDataset(val_df, BUCKET, IMAGES_PREFIX, s3, transform),\n",
    "    batch_size=32, shuffle=False, num_workers=2, collate_fn=collate_skip_missing)\n",
    "\n",
    "# --- Model ---\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * (img_size // 8) * (img_size // 8), 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "# --- Training ---\n",
    "print(\"training\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ConvNet().to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "print(type(model))\n",
    "print(isinstance(model, nn.Module))\n",
    "print(hasattr(model, 'parameters'))\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "num_epochs = 2\n",
    "loss_history = []\n",
    "epoch_loss_history = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss, samples = 0.0, 0\n",
    "    start = time.time()\n",
    "    print(\"Epoch \", {epoch+1})\n",
    "    for batch in trainloader:\n",
    "        if batch is None: \n",
    "            continue\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to(device), labels.unsqueeze(1).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_history.append(loss.item())\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        samples += inputs.size(0)\n",
    "\n",
    "    if samples == 0:\n",
    "        print(f\" Epoch {epoch+1}: No valid images were loaded. Skipping this epoch.\")\n",
    "        continue\n",
    "    \n",
    "    epoch_loss = running_loss / samples\n",
    "    epoch_loss_history.append(epoch_loss)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f} - Time: {time.time()-start:.1f}s\")\n",
    "\n",
    "# Save model + losses\n",
    "\n",
    "torch.save(model.state_dict(), \"model/effusion1_cnn.pth\")\n",
    "np.save(\"model/loss_history1.npy\", np.array(loss_history))\n",
    "np.save(\"model/epoch_loss_history1.npy\", np.array(epoch_loss_history))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(epoch_loss_history)\n",
    "plt.title(\"Epoch Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.savefig(\"model/train_loss_curve.png\")\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7ff7f9-8dde-4f02-b184-9301f47d8236",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test.py\n",
    "import torch, numpy as np, matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from train import ConvNet, S3ImageDataset, collate_skip_missing, transform, s3, BUCKET, IMAGES_PREFIX, labels_df\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- Load trained model ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ConvNet().to(device)\n",
    "model.load_state_dict(torch.load(\"model/effusion1_cnn.pth\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# --- Prepare test data ---\n",
    "_, val_df = train_test_split(labels_df, test_size=0.2, stratify=labels_df[\"label\"], random_state=42)\n",
    "valloader = DataLoader(\n",
    "    S3ImageDataset(val_df, BUCKET, IMAGES_PREFIX, s3, transform),\n",
    "    batch_size=32, shuffle=False, num_workers=2, collate_fn=collate_skip_missing\n",
    ")\n",
    "\n",
    "# --- Metrics containers ---\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "val_loss, samples = 0.0, 0\n",
    "all_labels, all_preds, all_probs = [], [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in valloader:\n",
    "        if batch is None: continue\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to(device), labels.unsqueeze(1).to(device)\n",
    "        logits = model(inputs)\n",
    "        loss = criterion(logits, labels)\n",
    "        val_loss += loss.item() * inputs.size(0)\n",
    "        samples += inputs.size(0)\n",
    "\n",
    "        probs = torch.sigmoid(logits).squeeze(1).cpu().numpy()\n",
    "        preds = (probs > 0.5).astype(int)\n",
    "        labs = labels.squeeze(1).cpu().numpy().astype(int)\n",
    "\n",
    "        all_probs.extend(probs.tolist())\n",
    "        all_preds.extend(preds.tolist())\n",
    "        all_labels.extend(labs.tolist())\n",
    "\n",
    "# --- Compute metrics ---\n",
    "val_loss /= samples\n",
    "all_labels = np.array(all_labels)\n",
    "all_preds = np.array(all_preds)\n",
    "val_acc = (all_preds == all_labels).mean()\n",
    "val_prec = precision_score(all_labels, all_preds, zero_division=0)\n",
    "val_rec = recall_score(all_labels, all_preds, zero_division=0)\n",
    "val_f1 = f1_score(all_labels, all_preds, zero_division=0)\n",
    "\n",
    "print(f\"\\nValidation Metrics:\")\n",
    "print(f\"Loss={val_loss:.4f} | Acc={val_acc:.4f} | Prec={val_prec:.4f} | Rec={val_rec:.4f} | F1={val_f1:.4f}\")\n",
    "\n",
    "# --- Save metrics and plots ---\n",
    "np.save(\"model/val_probs1.npy\", all_probs)\n",
    "np.save(\"model/val_preds1.npy\", all_preds)\n",
    "np.save(\"model/val_labels1.npy\", all_labels)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(all_probs, bins=30, alpha=0.7)\n",
    "plt.title(\"Predicted Probabilities\")\n",
    "plt.xlabel(\"Probability of Effusion\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.savefig(\"model/test_probs_hist.png\")\n",
    "\n",
    "print(\"Test results + plots saved in model/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "15bbe10e-ca18-4e10-abe8-d50419372aed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T15:38:50.988586Z",
     "iopub.status.busy": "2025-10-26T15:38:50.988346Z",
     "iopub.status.idle": "2025-10-26T15:40:27.583229Z",
     "shell.execute_reply": "2025-10-26T15:40:27.582517Z",
     "shell.execute_reply.started": "2025-10-26T15:38:50.988557Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 7725 rows from CSV\n",
      "üöÄ Starting training...\n",
      "Epoch 1/2 - Loss: 0.6948 - Time: 49.0s\n",
      "Epoch 2/2 - Loss: 0.6849 - Time: 47.4s\n",
      "‚úÖ Model weights saved to model/effusion1_cnn.pth\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import boto3, io, pandas as pd\n",
    "from PIL import Image\n",
    "from botocore.exceptions import ClientError\n",
    "import time, matplotlib.pyplot as plt, numpy as np\n",
    "import os\n",
    "\n",
    "# --- AWS setup ---\n",
    "REGION = \"us-east-1\"\n",
    "BUCKET = \"aws-chest-xray-model-hackathon\"\n",
    "IMAGES_PREFIX = \"archive\"\n",
    "LABELS_PATH = f\"s3://{BUCKET}/archive/effusion.csv\"\n",
    "\n",
    "s3 = boto3.client(\"s3\", region_name=REGION)\n",
    "\n",
    "# --- Load CSV ---\n",
    "labels_df = pd.read_csv(LABELS_PATH)\n",
    "labels_df.rename(columns={\"Image Index\": \"image_name\", \"Label\": \"label\"}, inplace=True)\n",
    "print(f\"‚úÖ Loaded {len(labels_df)} rows from CSV\")\n",
    "\n",
    "# --- Transform ---\n",
    "img_size = 256\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# --- Dataset ---\n",
    "class S3ImageDataset(Dataset):\n",
    "    def __init__(self, df, bucket, prefix, s3_client, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.bucket = bucket\n",
    "        self.prefix = prefix\n",
    "        self.s3 = s3_client\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image_name = str(row[\"image_name\"])\n",
    "        label = torch.tensor(row[\"label\"], dtype=torch.float32)\n",
    "        key = f\"{self.prefix}/{image_name}\"\n",
    "\n",
    "        try:\n",
    "            obj = self.s3.get_object(Bucket=self.bucket, Key=key)\n",
    "        except ClientError:\n",
    "            return None\n",
    "\n",
    "        img_bytes = obj[\"Body\"].read()\n",
    "        img = Image.open(io.BytesIO(img_bytes)).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "def collate_skip_missing(batch):\n",
    "    batch = [x for x in batch if x is not None]\n",
    "    if len(batch) == 0:\n",
    "        return None\n",
    "    imgs, labels = zip(*batch)\n",
    "    return torch.stack(imgs, 0), torch.tensor(labels)\n",
    "\n",
    "# --- Split dataset ---\n",
    "train_df, val_df = train_test_split(labels_df, test_size=0.2, stratify=labels_df[\"label\"], random_state=42)\n",
    "\n",
    "trainloader = DataLoader(\n",
    "    S3ImageDataset(train_df, BUCKET, IMAGES_PREFIX, s3, transform),\n",
    "    batch_size=16, shuffle=True, num_workers=2, collate_fn=collate_skip_missing)\n",
    "\n",
    "valloader = DataLoader(\n",
    "    S3ImageDataset(val_df, BUCKET, IMAGES_PREFIX, s3, transform),\n",
    "    batch_size=16, shuffle=False, num_workers=2, collate_fn=collate_skip_missing)\n",
    "\n",
    "# --- Model ---\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * (img_size // 8) * (img_size // 8), 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "# --- Train ---\n",
    "print(\"üöÄ Starting training...\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ConvNet().to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "num_epochs = 2  # (Keep small for now)\n",
    "loss_history = []\n",
    "\n",
    "os.makedirs(\"model\", exist_ok=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss, samples = 0.0, 0\n",
    "    start = time.time()\n",
    "    for batch in trainloader:\n",
    "        if batch is None: \n",
    "            continue\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to(device), labels.unsqueeze(1).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        samples += inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / samples\n",
    "    loss_history.append(epoch_loss)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f} - Time: {time.time()-start:.1f}s\")\n",
    "\n",
    "# Save model weights\n",
    "torch.save(model.state_dict(), \"model/effusion1_cnn.pth\")\n",
    "print(\"‚úÖ Model weights saved to model/effusion1_cnn.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "527efb46-66ea-4d36-bb75-3a8a20473d9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T15:40:31.980334Z",
     "iopub.status.busy": "2025-10-26T15:40:31.980103Z",
     "iopub.status.idle": "2025-10-26T15:40:32.172808Z",
     "shell.execute_reply": "2025-10-26T15:40:32.172204Z",
     "shell.execute_reply.started": "2025-10-26T15:40:31.980315Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "effusion1_cnn.pth\n"
     ]
    }
   ],
   "source": [
    "!ls model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7f5570dc-64be-48e7-a589-ab4cacd132e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T15:40:34.652490Z",
     "iopub.status.busy": "2025-10-26T15:40:34.652266Z",
     "iopub.status.idle": "2025-10-26T15:40:34.657141Z",
     "shell.execute_reply": "2025-10-26T15:40:34.656544Z",
     "shell.execute_reply.started": "2025-10-26T15:40:34.652471Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile inference.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import io, json\n",
    "\n",
    "# --- Model Definition (must match your training one) ---\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * (256 // 8) * (256 // 8), 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "\n",
    "# --- Required by SageMaker ---\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"Load the model for inference\"\"\"\n",
    "    model = ConvNet()\n",
    "    model.load_state_dict(torch.load(f\"{model_dir}/effusion1_cnn.pth\", map_location=\"cpu\"))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def input_fn(request_body, content_type):\n",
    "    \"\"\"Convert the input image bytes into a tensor\"\"\"\n",
    "    if content_type == \"application/x-image\":\n",
    "        image = Image.open(io.BytesIO(request_body)).convert(\"RGB\")\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        return transform(image).unsqueeze(0)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported content type: {content_type}\")\n",
    "\n",
    "\n",
    "def predict_fn(input_data, model):\n",
    "    \"\"\"Run the model and return the prediction\"\"\"\n",
    "    with torch.no_grad():\n",
    "        output = model(input_data)\n",
    "        prob = torch.sigmoid(output).item()\n",
    "        prediction = \"Yes\" if prob > 0.5 else \"No\"\n",
    "        return {\"prediction\": prediction, \"confidence\": prob}\n",
    "\n",
    "\n",
    "def output_fn(prediction_output, accept):\n",
    "    \"\"\"Return the result as JSON\"\"\"\n",
    "    return json.dumps(prediction_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "313842e5-696e-4a8b-93c6-296328c49676",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T15:40:38.754483Z",
     "iopub.status.busy": "2025-10-26T15:40:38.754234Z",
     "iopub.status.idle": "2025-10-26T15:40:38.947769Z",
     "shell.execute_reply": "2025-10-26T15:40:38.947202Z",
     "shell.execute_reply.started": "2025-10-26T15:40:38.754457Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference.py  model  model.tar.gz  outputs  test_image.png  xraymodel.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3f3121ec-8191-4c85-b667-c8934ae16345",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T15:40:42.928151Z",
     "iopub.status.busy": "2025-10-26T15:40:42.927914Z",
     "iopub.status.idle": "2025-10-26T15:40:44.606519Z",
     "shell.execute_reply": "2025-10-26T15:40:44.605947Z",
     "shell.execute_reply.started": "2025-10-26T15:40:42.928132Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ model.tar.gz created successfully!\n",
      "-rw-r--r-- 1 sagemaker-user users 30M Oct 26 15:40 model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "\n",
    "with tarfile.open(\"model.tar.gz\", \"w:gz\") as tar:\n",
    "    tar.add(\"inference.py\")\n",
    "    tar.add(\"model/effusion1_cnn.pth\", arcname=\"effusion1_cnn.pth\")\n",
    "\n",
    "print(\"‚úÖ model.tar.gz created successfully!\")\n",
    "!ls -lh model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "db9bd61a-6bb2-49ce-afda-e86ca259ded3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T15:40:49.003001Z",
     "iopub.status.busy": "2025-10-26T15:40:49.002751Z",
     "iopub.status.idle": "2025-10-26T15:40:49.485658Z",
     "shell.execute_reply": "2025-10-26T15:40:49.485062Z",
     "shell.execute_reply.started": "2025-10-26T15:40:49.002982Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded to s3://aws-chest-xray-model-hackathon/models/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "region = \"us-east-1\"\n",
    "bucket = \"aws-chest-xray-model-hackathon\"\n",
    "key = \"models/model.tar.gz\"\n",
    "\n",
    "s3 = boto3.client(\"s3\", region_name=region)\n",
    "s3.upload_file(\"model.tar.gz\", bucket, key)\n",
    "\n",
    "print(f\"‚úÖ Uploaded to s3://{bucket}/{key}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bba246e5-f762-4b3f-b34e-c40828ea017d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T15:40:53.587716Z",
     "iopub.status.busy": "2025-10-26T15:40:53.587489Z",
     "iopub.status.idle": "2025-10-26T15:40:53.614096Z",
     "shell.execute_reply": "2025-10-26T15:40:53.613511Z",
     "shell.execute_reply.started": "2025-10-26T15:40:53.587699Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Region: us-east-1\n",
      "‚úÖ Model artifact: s3://aws-chest-xray-model-hackathon/models/model.tar.gz\n",
      "‚úÖ Role: arn:aws:iam::902917435242:role/bedrock-workshop-studio-v2-SageMakerExecutionRole-r8PQyktIIzy5\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import time\n",
    "\n",
    "# ---- Configuration ----\n",
    "region = \"us-east-1\"\n",
    "bucket = \"aws-chest-xray-model-hackathon\"\n",
    "model_artifact = f\"s3://{bucket}/models/model.tar.gz\"\n",
    "\n",
    "model_name = \"xray-effusion-model\"\n",
    "endpoint_config_name = \"xray-effusion-config\"\n",
    "endpoint_name = \"xray-effusion-endpoint\"\n",
    "\n",
    "# SageMaker execution role (check this in your AWS console under SageMaker -> Roles)\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "print(\"‚úÖ Region:\", region)\n",
    "print(\"‚úÖ Model artifact:\", model_artifact)\n",
    "print(\"‚úÖ Role:\", role)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1299685d-ac62-4e52-936e-f1c8a132547a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T15:56:11.008664Z",
     "iopub.status.busy": "2025-10-26T15:56:11.008450Z",
     "iopub.status.idle": "2025-10-26T15:56:11.671850Z",
     "shell.execute_reply": "2025-10-26T15:56:11.671262Z",
     "shell.execute_reply.started": "2025-10-26T15:56:11.008648Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model created: effusion-cnn-model\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from time import sleep\n",
    "\n",
    "region = \"us-east-1\"\n",
    "model_name = \"effusion-cnn-model\"\n",
    "role_arn = \"arn:aws:iam::902917435242:role/bedrock-workshop-studio-v2-SageMakerExecutionRole-r8PQyktIIzy5\"\n",
    "model_data = \"s3://aws-chest-xray-model-hackathon/models/model.tar.gz\"\n",
    "\n",
    "sagemaker = boto3.client(\"sagemaker\", region_name=region)\n",
    "\n",
    "# ‚úÖ Use the correct PyTorch inference image\n",
    "container = {\n",
    "    \"Image\": \"763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference:2.0.0-cpu-py310-ubuntu20.04-sagemaker\",\n",
    "    \"ModelDataUrl\": model_data,\n",
    "}\n",
    "\n",
    "response = sagemaker.create_model(\n",
    "    ModelName=model_name,\n",
    "    ExecutionRoleArn=role_arn,\n",
    "    PrimaryContainer=container,\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Model created: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "70a52f03-fe1b-4f47-9c58-850f77fccca9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T15:57:40.386181Z",
     "iopub.status.busy": "2025-10-26T15:57:40.385943Z",
     "iopub.status.idle": "2025-10-26T15:57:40.767292Z",
     "shell.execute_reply": "2025-10-26T15:57:40.766764Z",
     "shell.execute_reply.started": "2025-10-26T15:57:40.386165Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Endpoint config created: effusion-cnn-config\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "sagemaker = boto3.client(\"sagemaker\", region_name=\"us-east-1\")\n",
    "\n",
    "response = sagemaker.create_endpoint_config(\n",
    "    EndpointConfigName=\"effusion-cnn-config\",\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "            \"ModelName\": \"effusion-cnn-model\",\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"InstanceType\": \"ml.m5.large\",  # good default for CPU inference\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Endpoint config created: effusion-cnn-config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "55667152-3970-404d-a99e-965fa449219c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T15:58:33.560820Z",
     "iopub.status.busy": "2025-10-26T15:58:33.560601Z",
     "iopub.status.idle": "2025-10-26T16:02:04.784704Z",
     "shell.execute_reply": "2025-10-26T16:02:04.784107Z",
     "shell.execute_reply.started": "2025-10-26T15:58:33.560803Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Creating endpoint: effusion-cnn-endpoint ... this may take ~10 minutes\n",
      "‚úÖ Endpoint is live and ready: effusion-cnn-endpoint\n"
     ]
    }
   ],
   "source": [
    "endpoint_name = \"effusion-cnn-endpoint\"\n",
    "\n",
    "response = sagemaker.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    ")\n",
    "\n",
    "print(f\"üöÄ Creating endpoint: {endpoint_name} ... this may take ~10 minutes\")\n",
    "\n",
    "# Wait for deployment to finish\n",
    "waiter = sagemaker.get_waiter(\"endpoint_in_service\")\n",
    "waiter.wait(EndpointName=endpoint_name)\n",
    "\n",
    "print(f\"‚úÖ Endpoint is live and ready: {endpoint_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "de036ae3-dec0-4610-94e6-966d56735c85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T16:02:18.537501Z",
     "iopub.status.busy": "2025-10-26T16:02:18.537289Z",
     "iopub.status.idle": "2025-10-26T16:02:18.643198Z",
     "shell.execute_reply": "2025-10-26T16:02:18.642617Z",
     "shell.execute_reply.started": "2025-10-26T16:02:18.537485Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Image downloaded from S3 ‚Üí test_image.png\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "# S3 setup\n",
    "bucket = \"aws-chest-xray-model-hackathon\"\n",
    "key = \"archive/00001338_003.png\"  # replace with any image key that exists in your bucket\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "s3.download_file(bucket, key, \"test_image.png\")\n",
    "\n",
    "print(\"‚úÖ Image downloaded from S3 ‚Üí test_image.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5c2b3b44-a998-47c5-b68a-cf24fccf79a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T16:02:22.660900Z",
     "iopub.status.busy": "2025-10-26T16:02:22.660673Z",
     "iopub.status.idle": "2025-10-26T16:02:22.918229Z",
     "shell.execute_reply": "2025-10-26T16:02:22.917735Z",
     "shell.execute_reply.started": "2025-10-26T16:02:22.660884Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Prediction result:\n",
      "{\n",
      "  \"prediction\": \"Yes\",\n",
      "  \"confidence\": 0.5406027436256409\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import boto3, json\n",
    "\n",
    "runtime = boto3.client(\"sagemaker-runtime\", region_name=\"us-east-1\")\n",
    "endpoint_name = \"effusion-cnn-endpoint\"\n",
    "\n",
    "# ‚ö†Ô∏è replace this with the actual path to a test image in your Jupyter environment\n",
    "test_image_path = \"test_image.png\"\n",
    "\n",
    "with open(test_image_path, \"rb\") as f:\n",
    "    payload = f.read()\n",
    "\n",
    "response = runtime.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"application/x-image\",\n",
    "    Body=payload,\n",
    ")\n",
    "\n",
    "result = json.loads(response[\"Body\"].read().decode())\n",
    "print(\"‚úÖ Prediction result:\")\n",
    "print(json.dumps(result, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5f2273-faac-42d2-8a0c-5ebce81ad724",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
